<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI/MachineLearning on Kirawat Sahasewiyon</title><link>https://kirawat.me/tags/ai/machinelearning/</link><description>Recent content in AI/MachineLearning on Kirawat Sahasewiyon</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://kirawat.me/tags/ai/machinelearning/index.xml" rel="self" type="application/rss+xml"/><item><title>Pre-training</title><link>https://kirawat.me/garden/pre-training/</link><pubDate>Sun, 07 Jan 2024 17:19:00 +0700</pubDate><guid>https://kirawat.me/garden/pre-training/</guid><description>Pre-training is a technique used in machine learning to train a model on a large corpus of data before fine-tuning it for a specific task. This approach has been shown to improve the performance of models on a wide range of tasks, including natural language processing, computer vision, and speech recognition.
Benefits of pre-training Improved performance: Pre-trained models can achieve higher accuracy and generalizability compared to models trained from scratch.</description></item><item><title>Epoch</title><link>https://kirawat.me/garden/epoch/</link><pubDate>Sun, 07 Jan 2024 16:44:00 +0700</pubDate><guid>https://kirawat.me/garden/epoch/</guid><description>In machine learning, an epoch refers to one complete pass of the entire training dataset through the neural network model. During each epoch, the model is exposed to all of the training examples and makes adjustments to its internal parameters in order to minimize the loss function. The number of epochs is a hyperparameter that can be tuned to optimize the performance of the model.
How does it work? Training a model on multiple epochs means repeatedly exposing it to the entire training dataset.</description></item></channel></rss>