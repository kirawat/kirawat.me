<!doctype html><html lang=en-us dir=ltr data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Voice Changing with Machine Learning | Kirawat Sahasewiyon</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Inter&family=Mate&family=Trirong:wght@300&display=swap"><link rel=stylesheet href=/css/style.min.04d62c8f7f04bf78ee8398f612cb4996123bbe10de4bdd42909e4ee08bdaa384.css integrity="sha256-BNYsj38Ev3jug5j2EstJlhI7vhDeS91CkJ5O4Ivao4Q=" crossorigin=anonymous><script src=/js/top.364772487a6086850096100cba86880e1b4ac8766d1da5dfd7b5c40483f2dc50.js integrity="sha256-NkdySHpghoUAlhAMuoaIDhtKyHZtHaXf17XEBIPy3FA=" crossorigin=anonymous></script></head><body><header><div id=header><div class=header-container><div class=site-title><a href=/>Kirawat Sahasewiyon</a></div><div class=spacer></div><div class=menu><nav><ul><li><a aria-current=true class=ancestor href=/garden/>Garden</a></li></ul></nav></div><div class=theme-slider-container><label class=theme-slider><input type=checkbox id=dark-mode-button onclick=toggleDarkMode()><div class="round slider"></div></label></div></div></div></header><main><div id=breadcrumbs><ul><li><a href=/>Home</a></li><li><a href=/garden/>Digital Garden</a></li></ul></div><div id=content-container><div class=post-container><aside class=sidebar-l><div class=toc><div class=title>Table of Contents</div><nav id=TableOfContents><ul><li><a href=#tools>Tools</a></li><li><a href=#vc-client>VC Client</a><ul><li><a href=#directml-or-cuda>DirectML or CUDA?</a></li><li><a href=#recommended-settings>Recommended Settings</a></li><li><a href=#audio-setup>Audio Setup</a></li><li><a href=#explanations>Explanations</a></li></ul></li><li><a href=#how-to-train-your-own-voice-model>How to train your own voice model</a><ul><li><a href=#total-training-epochs-total_epoch>Total training epochs (total_epoch)</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></aside><article><div class=header><h1 class=title>Voice Changing with Machine Learning</h1><div class=meta-container><div class=tags><ul><li><a href=/tags/ai/machinelearning/voiceconversion/>AI/MachineLearning/VoiceConversion</a></li></ul></div><div class=post-date-container><div class=post-date>Planted&nbsp;
<span id=planted-date data-datetime=2024-01-04T21:42:00+07:00></span>
<span class=tooltip>4 Jan 2024 21:42:00 PM (+07:00)</span></div><div class=post-date>Last tended&nbsp;
<span id=last-tended data-datetime=2024-01-08T11:19:26+07:00></span>
<span class=tooltip>8 Jan 2024 11:19:26 AM (+07:00)</span></div></div></div></div><div class=content><h2 id=tools>Tools</h2><ul><li><p><a class=external-link href=https://github.com/w-okada/voice-changer target=_blank>VC Client</a>: This is a client software for performing real-time voice conversion using various Voice Conversion (VC) AI.</p></li><li><p><a class=external-link href=https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI target=_blank>Retrieval-based-Voice-Conversation-WebUI</a>: A software for training <a href=../retrieval-based-voice-conversion-rvc>RVC (Retrieval-based Voice Conversion)</a> voice model.</p></li><li><p><a class=external-link href=https://vac.muzychenko.net/en/ target=_blank>Virtual Audio Cable (VAC)</a>: A software to output the generated voice from VC Client to the target software.</p></li></ul><p>Quality of your microphone and cable uses can impact the real-time voice conversion result.</p><h2 id=vc-client>VC Client</h2><h3 id=directml-or-cuda>DirectML or CUDA?</h3><blockquote class="callout note"><div class=title>TLDR;</div><p></p>If you have NVIDIA GPUs, go for CUDA. If you have AMD or INTEL ARC GPUs, go for DirectML. For macOS, there is only one version.</p></blockquote><p>DirectML and CUDA are both programming interfaces for running machine learning workloads on GPUs. However, they differ in several ways:</p><ul><li><p><strong>DirectML</strong> is a Microsoft-developed API that is part of the DirectX 12 graphics API. It is specifically designed for machine learning workloads and is optimized for DirectX-compatible GPUs.</p></li><li><p><strong>CUDA</strong> is an NVIDIA-developed API that is designed for general-purpose computing on NVIDIA GPUs. It is also widely used for machine learning workloads, but it is not as tightly integrated with DirectX as DirectML is.</p></li></ul><p>In general, DirectML is a good choice for machine learning workloads that are running on DirectX-compatible GPUs. It is also a good choice for workloads that are being developed using Microsoft&rsquo;s machine learning frameworks, such as TensorFlow and PyTorch.</p><p>CUDA is a more versatile API that can be used for a wider range of workloads. It is also a good choice for workloads that are being developed using NVIDIA&rsquo;s machine learning frameworks, such as TensorFlow and PyTorch, as well as other frameworks such as MXNet and PaddlePaddle.</p><h3 id=recommended-settings>Recommended Settings</h3><h4 id=advanced-setting>Advanced Setting</h4><ul><li><strong>Protocol:</strong> Sio or Rest (try both and see what you prefer)</li><li><strong>Crossfade:</strong> overlap 4096 start 0.2 end 0.8</li><li><strong>Trancate:</strong> 300</li><li><strong>SilenceFront:</strong> Off</li><li><strong>Protect:</strong> 0.5</li><li><strong>RVC Quality:</strong> Low (changing to high cranks GPU and CPU usage, for basically no real difference).</li><li><strong>Skip Pass through confirmation:</strong> No</li></ul><h3 id=audio-setup>Audio Setup</h3><h4 id=s-threshold>S. Threshold</h4><p>The minimum required sound for it to start converting audio. If you raise IN Gain at the top, nothing you do here will matter. If its below, it&rsquo;s considered silence, hence the name Silence Threshold.</p><h3 id=explanations>Explanations</h3><h4 id=chunk>Chunk</h4><blockquote><p>Decide how much length to cut and convert in one conversion. The higher the value, the more efficient the conversion, but the larger the buf value, the longer the maximum time before the conversion starts.<sup id=fnref:1><a href=../voice-changing-with-machine-learning//#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p></blockquote><blockquote><p>Chunk, the lower this is the less latency you have for the voice to come out, but the lower the quality becomes each time, you find the one where it doesn&rsquo;t have any audio glitches, i.e. stuttering repeating words, cutting in and out, or laggy.<sup id=fnref:2><a href=../voice-changing-with-machine-learning//#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p></blockquote><h4 id=extra>Extra</h4><blockquote><p>Determines how much past audio to include in the input when converting audio. The longer the past voice is, the better the accuracy of the conversion, but the longer the res is, the longer the calculation takes.<sup id=fnref1:2><a href=../voice-changing-with-machine-learning//#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p></blockquote><blockquote><p>Extra start with 4096 while testing your chunk values. The larger this value is the more CPU resources it uses. It probably zero point in going higher than 32768 value, but it can however make your voice clearer.<sup id=fnref2:2><a href=../voice-changing-with-machine-learning//#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p></blockquote><h4 id=tune>Tune</h4><blockquote><p>Tune is voice dependent so female to make you want a NEGATIVE tune usually -12, female to female you ideally don&rsquo;t have to change anything but you might have to depending on how soft your voice is in comparison. Male to female you want a POSITIVE tune usually +12.<sup id=fnref3:2><a href=../voice-changing-with-machine-learning//#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p></blockquote><h4 id=index>Index</h4><blockquote><p>Index is only really beneficial if your Accent is HEAVY, or DOESN&rsquo;T MATCH the person accent you want. But the cost is CPU usage. 300% more usage to be exact. It is recommend not using this, and just speaking naturally.<sup id=fnref4:2><a href=../voice-changing-with-machine-learning//#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p></blockquote><h2 id=how-to-train-your-own-voice-model>How to train your own voice model</h2><p>Download <a class=external-link href=https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI target=_blank>Retrieval-based-Voice-Conversation-WebUI</a>, then open <code>go-web.bat</code> file.</p><p>According to a <a class=external-link href=https://rentry.co/VoiceChangerGuide target=_blank>Guide for W-Okada&rsquo;s RealTimeVoiceChangerClient</a> by Raven:</p><blockquote><p>A dataset of around 10 minutes to 50 minutes is recommended.</p></blockquote><h3 id=total-training-epochs-total_epoch>Total training epochs (total_epoch)</h3><p>According to <a class=external-link href=https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/FAQ-%28Frequently-Asked-Questions%29#q9how-many-total_epoch-are-optimal target=_blank>FAQ</a> from RVC-Project/Retrieval-based-Voice-Conversion-WebUI on GitHub<sup id=fnref:3><a href=../voice-changing-with-machine-learning//#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>:</p><blockquote><p>If the training dataset&rsquo;s audio quality is poor and the noise floor is high, 20-30 <a href=../epoch>epochs</a> are sufficient. Setting it too high won&rsquo;t improve the audio quality of your low-quality training set.</p><p>If the training set audio quality is high, the noise floor is low, and there is sufficient duration, you can increase it. 200 is acceptable (since training is fast, and if you&rsquo;re able to prepare a high-quality training set, your GPU likely can handle a longer training duration without issue).</p></blockquote><h2 id=references>References</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a class=external-link href=https://github.com/w-okada/voice-changer/blob/master/tutorials/tutorial_rvc_en_latest.md target=_blank>&ldquo;Realtime Voice Changer Client for RVC Tutorial (v.1.5.3.13)&rdquo;</a>. <em>w-okada/voice-changer</em>, <em>GitHub</em>. Retrieved January 4, 2024.&#160;<a href=../voice-changing-with-machine-learning//#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Raven (December 29, 2023). <a class=external-link href=https://rentry.co/VoiceChangerGuide target=_blank>&ldquo;Guide for W-Okada&rsquo;s RealTimeVoiceChangerClient&rdquo;</a>. <em>Rentry.co</em>. <a class=external-link href=https://web.archive.org/web/20240104151243/https://rentry.co/VoiceChangerGuide target=_blank>Archived</a> from the original on January 4, 2024. Retrieved January 4, 2024.&#160;<a href=../voice-changing-with-machine-learning//#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=../voice-changing-with-machine-learning//#fnref1:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=../voice-changing-with-machine-learning//#fnref2:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=../voice-changing-with-machine-learning//#fnref3:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=../voice-changing-with-machine-learning//#fnref4:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>RVC-Project/Retrieval-based-Voice-Conversion-WebUI (August 31, 2023). <a class=external-link href=https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/FAQ-%28Frequently-Asked-Questions%29#q9how-many-total_epoch-are-optimal target=_blank>&ldquo;FAQ (Frequently Asked Questions)&rdquo;</a>. <em>GitHub</em>. <a class=external-link href=https://web.archive.org/web/20240104152438/https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/FAQ-%28Frequently-Asked-Questions%29 target=_blank>Archived</a> from the original on January 4, 2024. Retrieved January 4, 2024.&#160;<a href=../voice-changing-with-machine-learning//#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article><aside class=sidebar-r></aside></div><div class=comments><div class=giscus></div></div></div></main><footer><div class=container><div class=copyright>Â© 2024 Kirawat Sahasewiyon.</div><div class=spacer></div><span class=license>&nbsp;Content licensed <a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.4/moment.min.js></script>
<script src=/js/main.d11c9e7c947eb891322a28811fcb360a7b3709ea322be03e8b10522f51da670f.js integrity="sha256-0RyefJR+uJEyKiiBH8s2Cns3CeoyK+A+ixBSL1HaZw8=" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9CXL356JPY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-9CXL356JPY",{anonymize_ip:!1})}</script><script>const theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");if(theme){const t={src:"https://giscus.app/client.js","data-repo":"kirawat/kirawat.me","data-repo-id":"R_kgDOK5SkpQ","data-category":"Comments","data-category-id":"DIC_kwDOK5Skpc4Cb12k","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":theme,"data-lang":"en","data-loading":"lazy",crossorigin:"anonymous",async:""};let e=document.createElement("script");Object.entries(t).forEach(([t,n])=>e.setAttribute(t,n)),document.body.appendChild(e)}</script></body></html>