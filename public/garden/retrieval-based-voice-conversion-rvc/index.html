<!doctype html><html lang=en-us dir=ltr data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Retrieval-based Voice Conversion (RVC) | Kirawat Sahasewiyon</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Inter&family=Mate&family=Trirong:wght@300&display=swap"><link rel=stylesheet href=/css/style.min.f0c2dbeece17846d7f4cb782102668aba693522052c117223ce4595100f4f208.css integrity="sha256-8MLb7s4XhG1/TLeCECZoq6aTUiBSwRciPORZUQD08gg=" crossorigin=anonymous><script src=/js/top.364772487a6086850096100cba86880e1b4ac8766d1da5dfd7b5c40483f2dc50.js integrity="sha256-NkdySHpghoUAlhAMuoaIDhtKyHZtHaXf17XEBIPy3FA=" crossorigin=anonymous></script></head><body><header><div id=header><div class=header-container><div class=site-title><a href=/>Kirawat Sahasewiyon</a></div><div class=spacer></div><div class=menu><nav><ul><li><a aria-current=true class=ancestor href=/garden/>Garden</a></li></ul></nav></div><div class=theme-slider-container><label class=theme-slider><input type=checkbox id=dark-mode-button onclick=toggleDarkMode()><div class="round slider"></div></label></div></div></div></header><main><div id=breadcrumbs><ul><li><a href=/>Home</a></li><li><a href=/garden/>Digital Garden</a></li></ul></div><div id=content-container><div class=post-container><aside class=sidebar-l><div class=toc><div class=title>Table of Contents</div><nav id=TableOfContents><ul><li><a href=#advantages-of-rvc>Advantages of RVC</a></li><li><a href=#overview-of-how-rvc-works>Overview of how RVC works</a></li><li><a href=#tutorial>Tutorial</a><ul><li><a href=#learning-time>Learning Time</a></li><li><a href=#model>Model</a></li><li><a href=#pretrained-model>Pretrained model</a></li><li><a href=#learning-index>Learning index</a></li><li><a href=#loss-terms>Loss Terms</a></li></ul></li><li><a href=#see-also>See also</a></li></ul></nav></div></aside><article><div class=header><h1 class=title>Retrieval-based Voice Conversion (RVC)</h1><div class=meta-container><div class=tags><ul><li><a href=/tags/ai/machinelearning/voiceconversion/>AI/MachineLearning/VoiceConversion</a></li></ul></div><div class=post-date-container><div class=post-date>Planted&nbsp;
<span id=planted-date data-datetime=2024-01-07T17:01:00+07:00></span>
<span class=tooltip>7 Jan 2024 17:01:00 PM (+07:00)</span></div><div class=post-date>Last tended&nbsp;
<span id=last-tended data-datetime=2024-01-07T18:44:16+07:00></span>
<span class=tooltip>7 Jan 2024 18:44:16 PM (+07:00)</span></div></div></div></div><div class=content><p>Retrieval-based voice conversion (RVC) is a method of voice cloning that uses a pre-trained model to retrieve and combine segments of speech from a source speaker to synthesize the voice of a target speaker. Unlike other voice cloning methods, RVC does not require a large amount of training data from the target speaker. This makes it a more versatile and efficient approach to voice cloning.</p><p>Traditional voice conversion techniques typically rely on statistical models, such as hidden Markov models (HMMs) or Gaussian mixture models (GMMs), to learn the mapping between the source and target voices. These models are trained on a large corpus of parallel data, which consists of pairs of audio recording from the source and target speakers.</p><p>RVC, on the other hand, does not require parallel data. Instead, it uses a retrieval-based approach to find the most similar audio segments from the target speaker&rsquo;s voice and then uses these segments to synthesize the converted voice. This approach is more efficient and less data-intensive than traditional voice conversion techniques.</p><p>In addition to the differences in data requirements and efficiency, RVC also offers several other advantages over traditional voice conversion techniques. For example, RVC is more robust to noise and distortions in the source voice. It is also versatile and can be used to convert voices to a wider range of target styles.</p><h2 id=advantages-of-rvc>Advantages of RVC</h2><p>Here are some of the specific advantages of RVC:</p><ul><li><p><strong>Noise robustness:</strong> RVC is less sensitive to noise in the source voice than traditional voice conversion techniques. This is because RVC does not rely on statistical models, which can be easily corrupted by noise.</p></li><li><p><strong>Style transfer:</strong> RVC can be used to convert voices to a wider range of target styles than traditional voice conversion techniques. This is because RVC can leverage the entire range of styles represented in the target speaker&rsquo;s voice.</p></li><li><p><strong>Expressiveness:</strong> RVC can capture the expressiveness of the source speaker&rsquo;s voice, including emotions and intonation. This is because RVC is based on retrieving the most similar audio segments from the target speaker&rsquo;s voice, which preserves the natural expressiveness of the target speaker.</p></li></ul><p>RVC offers several advantages over other voice cloning methods:</p><ul><li><p><strong>Less training data required:</strong> RVC can be trained with a relatively small amount of data from the target speaker, typically less than 10 minutes of speech.</p></li><li><p><strong>Efficient and versatile:</strong> RVC can be used to convert a wide range of voices, including those with accents or speech impediments.</p></li><li><p><strong>High quality results:</strong> RVC can produce synthesized speech that is very similar to the target speaker&rsquo;s original voice.</p></li></ul><p>However, RVC also has some limitations:</p><ul><li><p><strong>Tone leakage:</strong> In some cases, the synthesized speech may retain some of the source speaker&rsquo;s voice characteristics.</p></li><li><p><strong>Limited expressiveness:</strong> RVC may not be able to capture the full expressiveness of the target speaker&rsquo;s voice.</p></li></ul><h2 id=overview-of-how-rvc-works>Overview of how RVC works</h2><p>In retrieval-based voice conversion (RVC), the model learns to clone a voice by retrieving and combining audio segments from the target speaker&rsquo;s voice. This process involves several steps:</p><ol><li><p><strong>Pre-training:</strong> A large corpus of speech data from a variety of speakers is used to train a neural network model. This model learns to encode speech into a high-dimensional representation that captures the acoustic characteristics of each speaker&rsquo;s voice. See <a href=../pre-training>Pre-training</a> for more info.</p></li><li><p><strong>Feature Extraction:</strong> The model extracts acoustic features from both the source and target speaker&rsquo;s voice. These features represent the characteristics of the speech signal, such as pitch, loudness, and spectral content.</p></li><li><p><strong>Speaker Embeddings:</strong> The model learns speaker embeddings, which are vector representations that capture the unique characteristics of each speaker&rsquo;s voice. These embeddings are used to identify the most similar audio segments from the target speaker&rsquo;s voice.</p></li><li><p><strong>Segment Retrieval:</strong> For each utterance in the source voice, the model retrieves the most similar audio segments from the target speaker&rsquo;s voice. The similarity is determined based on the acoustic features and speaker embeddings.</p></li><li><p><strong>Segment Combination:</strong> The retrieved segments are then combined using a waveform synthesis technique, such as overlap-add or phase vocoder, to generate the converted voice. This process ensures that the converted voice maintains the overall intonation and rhythm of the source speaker while adopting the target speaker&rsquo;s voice characteristics.</p></li><li><p><strong>Fine-tuning:</strong> The model is fine-tuned using a loss function that measures the similarity between the converted voice and the target speaker&rsquo;s voice. This fine-tuning process helps to improve the quality of the converted voice.</p></li></ol><p>Through this process, the model learns to adjust the acoustic features and speaker embeddings of the source speaker&rsquo;s voice to match those of the target speaker. This allows the model to effectively clone the target speaker&rsquo;s voice.</p><h2 id=tutorial>Tutorial</h2><p>Here is a tutorial on using <a class=external-link href=https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI target=_blank>Retrieval-based-Voice-Conversion-WebUI</a> to train voice conversion model. It comes with a dataset for the pre-training model that uses nearly 50 hours of high quality audio from the VCTK open source dataset.</p><h3 id=learning-time>Learning Time</h3><p>In deep learning, the data set is divided and the learning proceeds little by little. In one model update (step), <code>batch_size</code> data are retrieved and predictions and error corrections are performed. Doing this once for a dataset counts as one <a href=../epoch>epoch</a>.</p><p>Therefore, the learning time is the learning time per step x (the number of data in the dataset / batch size) x the number of epochs. In general, the larger the batch size, the more stable the learning becomes (learning time per step ÷ batch size) becomes smaller, but it uses more GPU memory. GPU RAM can be checked with the <code>nvidia-smi</code> command. Learning can be done in a short time by increasing the batch size as much as possible according to the machine of the execution environment.</p><h3 id=model>Model</h3><p>In the context of generative adversarial networks (GANs), the &ldquo;G&rdquo; and &ldquo;D&rdquo; files represent the generator and discriminator models, respectively. These two models are the core components of a GAN and play crucial roles in generating and evaluating synthetic data.</p><p><strong>Generator (G):</strong> The generator model is responsible for creating new data samples that resemble the real data distribution. It takes a random noise vector as input and transforms it into a synthetic data sample. The generator&rsquo;s objective is to produce data that is indistinguishable from the real data, making it difficult for the discriminator to differentiate between them.</p><p><strong>Discriminator (D):</strong> The discriminator model is tasked with distinguishing between real and synthetic data samples. It takes a data sample as input and outputs a probability indicating how likely it is that the sample is real. The discriminator&rsquo;s objective is to accurately classify real and synthetic data, helping to improve the generator&rsquo;s performance.</p><p>The G and D models are trained in an adversarial manner, where the generator tries to fool the discriminator, and the discriminator tries to catch the generator. This adversarial process drives the generator to produce increasingly realistic synthetic data.</p><p>The specific implementation and training details of the G and D models can vary depending on the specific GAN architecture and application. However, the fundamental roles of these two models remain the same: the generator creates synthetic data, and the discriminator evaluates its quality.</p><h3 id=pretrained-model>Pretrained model</h3><p>RVC starts training the model from <a href=../pre-training>pretrained weights</a> instead of from 0, so it can be trained with a small dataset.</p><p>By default it loads <code>rvc-location/pretrained/f0G40k.pth</code> and <code>rvc-location/pretrained/f0D40k.pth</code>.</p><p>When learning, model parameters are saved in <code>logs/{experiment name}/G_{}.pth</code>and <code>logs/{experiement name}/D_{}.pth</code> for each <code>save_every_epoch</code>.</p><p>You can restart or start training from model weights learned in a different experiment by point the path to that model instead of using the default pretrained model.</p><h3 id=learning-index>Learning index</h3><p>RVC saves the HuBERT feature values used during training, and during inference, searches for feature values that are similar to the feature values used during learning to perform inference. In order to perform this search at high speed, the index is learned in advance. For index learning, RVC use the approximate neighborhood search library faiss. Read the feature value of <code>/logs/{experiment name}/3_feature768</code>, save the combined feature value as <code>/logs/{experiement name}/total_fea.npy</code>, and use it to lean the index <code>/logs/{experiment name}/added_XXX.index</code>.</p><h3 id=loss-terms>Loss Terms</h3><p>Loss terms provide valuable feedback during the training process of GANs, helping to improve the quality and realism of the generated synthetic data.</p><p>Monitoring these loss terms together provides a comprehensive picture of the training progress. A successful training run would typically exhibit a balance between decreasing generator and discriminator losses, along with decreasing feature matching, mel spectrogram, and KL divergence losses. This indicates that the generator is producing increasingly realistic synthetic data while maintaining a similar underlying structure and distribution to real data.</p><p>The optimal values will vary depending on the specific task and dataset. However, by monitoring the trends in these loss terms, you can gain valuable insights into the progress of the training and identify any potential issues that may arise.</p><h4 id=discriminator-loss-loss_disc>Discriminator Loss (loss_disc):</h4><p>The discriminator loss measures the discriminator&rsquo;s ability to distinguish between real and synthetic data samples. It is typically formulated as a cross-entropy loss, where the discriminator is penalized for misclassifying real and synthetic samples.</p><p>A decreasing discriminator loss indicates that the discriminator is becoming better at distinguishing between real and synthetic data. This is generally a good sign, as it suggests that the generator is producing increasingly realistic synthetic data. However, if the discriminator loss becomes too low, it may indicate that the generator is simply memorizing real data samples rather than learning to capture the underlying patterns in the data.</p><h4 id=generator-loss-loss_gen>Generator Loss (loss_gen):</h4><p>The generator loss measures the generator&rsquo;s ability to produce synthetic data samples that are indistinguishable from real data. It is typically formulated as an adversarial loss, where the generator is penalized for producing data that is easily classified as synthetic by the discriminator.</p><p>A decreasing generator loss indicates that the generator is successful learning to producing realistic synthetic data.</p><h4 id=feature-matching-loss-loss_fm>Feature Matching Loss (loss_fm):</h4><p>The feature matching loss encourage the generator to produce synthetic data that has similar intermediate feature representations to those of real data. This is achieved by comparing the intermediate activations of a feature extractor applied to both real and synthetic data.</p><p>A decreasing feature matching loss indicates that the generator is producing synthetic data with similar intermediate feature representations to those of real data. This suggests that the generator is learning to capture the underlying structure of real data, which contributes to the realism of the synthetic data.</p><h4 id=mel-spectrogram-loss-loss_mel>Mel Spectrogram Loss (loss_mel):</h4><p>The mel spectrogram loss compares the mel spectrograms of real and synthetic data samples. Mel spectrograms are a way of representing the frequency content of an audio signal, and this loss encourage the generator to produce synthetic data that sound similar to real data.</p><p>A decreasing mel spectrogram loss indicates that the generator is producing synthetic data with a similar spectral distribution to real data. This is particularly important for audio generation tasks, as it ensures that the synthetic audio sound similar to real audio.</p><h4 id=kl-divergence-loss-loss_kl>KL Divergence Loss (loss_kl):</h4><p>The KL divergence loss encourages the generator to produce synthetic data that has a similar distribution of latest variables to real data. Latent variables are a representation of the underlying factors that generate the data, and this loss ensures that the generator is not simply memorizing real data samples but is learning to capture the underlying patterns in the data.</p><p>A decreasing KL divergence loss indicates that the generator is producing synthetic data with a similar distribution of latent variables to real data. This suggests that the generator is not simply memorizing real data samples but is learning to capture the underlying patterns in the data, which leads to more generalizable synthetic data.</p><h2 id=see-also>See also</h2><ul><li><a href=../voice-changing-with-machine-learning>Voice Changing with Machine Learning</a></li></ul></div></article><aside class=sidebar-r></aside></div><div class=comments><div class=giscus></div></div></div></main><footer><div class=container><div class=copyright>© 2024 Kirawat Sahasewiyon.</div><div class=spacer></div><span class=license>&nbsp;Content licensed <a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.4/moment.min.js></script>
<script src=/js/main.d11c9e7c947eb891322a28811fcb360a7b3709ea322be03e8b10522f51da670f.js integrity="sha256-0RyefJR+uJEyKiiBH8s2Cns3CeoyK+A+ixBSL1HaZw8=" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9CXL356JPY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-9CXL356JPY",{anonymize_ip:!1})}</script><script>const theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");if(theme){const t={src:"https://giscus.app/client.js","data-repo":"kirawat/kirawat.me","data-repo-id":"R_kgDOK5SkpQ","data-category":"Comments","data-category-id":"DIC_kwDOK5Skpc4Cb12k","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":theme,"data-lang":"en","data-loading":"lazy",crossorigin:"anonymous",async:""};let e=document.createElement("script");Object.entries(t).forEach(([t,n])=>e.setAttribute(t,n)),document.body.appendChild(e)}</script></body></html>